{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning label model conditioned on disagreements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import f1_score\n",
    "from CRF import run_crf, compute_dice\n",
    "from label_model import run_seg_label_model, generate_data, flying_squid, binarize_conditionals, compute_baselines\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook overview\n",
    "\n",
    "This notebook will estimate segmentation masks give noisy outputs of labeling functions. We currently assume five labeling functions are available to generate five noisy segmentation masks per training image.\n",
    "\n",
    "We first include an example with synthetic data to make sure everything is working, then show an example with real data. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test PGM estimation with synthetic data\n",
    "\n",
    "Here, we include an example with synthetic data. This should validate the code is working. An example with real data is in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate synthetic data according to proposed PGM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a few vars controlling dataset size and distribution\n",
    "n_train=500000 # Number of samples in synthetic train set\n",
    "n_test=100000 # Number of samples in synthetic val set\n",
    "use_comp = True # Which data generating function to use. False for simple, classification-like PGM; True for complex, segmentation-like PGM\n",
    "\n",
    "# Some additional parameters, do not change\n",
    "dep_nodes = [0,1,2] # Nodes among which we check for disagreements\n",
    "ind_nodes = [3,4] # Nodes we keep conditionally independent for triplet method\n",
    "all_nodes = dep_nodes + ind_nodes\n",
    "n_conds = 4 # Number of disagreement conditions\n",
    "\n",
    "# Generate some random parameters we'll use to define the ground truth data distribution\n",
    "if not use_comp: # Synthetic data for standard WS PGM\n",
    "    theta_y = .05*np.random.randn(1,1)\n",
    "    theta_lam_y_ind = np.random.uniform(.1,1,(len(all_nodes),1))\n",
    "    theta_lam_y_cond = None\n",
    "    theta_lam_lam = None\n",
    "else: # Synthetic data matching complex PGM\n",
    "    std_conds = 0.2\n",
    "    theta_y = .05*np.random.randn() + np.random.uniform(-std_conds,std_conds,(n_conds,1))\n",
    "    theta_lam_y_ind = np.random.uniform(0.2,0.6,len(ind_nodes)) + np.random.uniform(-std_conds,std_conds,(n_conds,len(ind_nodes)))\n",
    "    theta_lam_y_cond = np.random.uniform(0.2,0.6) + np.random.uniform(-std_conds,std_conds,(n_conds))\n",
    "    theta_lam_lam = None    \n",
    "thetas = [theta_y,theta_lam_y_ind,theta_lam_y_cond,theta_lam_lam]\n",
    "\n",
    "# Generate a dataset according to canonical parameters (thetas)\n",
    "sample_matrix, sample_matrix_test, lst, pmf = generate_data(n_train, n_test, theta_y, theta_lam_y_ind, theta_lam_y_cond, theta_lam_lam, m=len(all_nodes), v=len(all_nodes)+1, comp=use_comp)\n",
    "\n",
    "# View data\n",
    "fig, axs = plt.subplots(1,3,figsize=(18,5))\n",
    "axs[0].plot(np.mean(np.asarray(lst)[:,:-1]==1,1),pmf,'r.')\n",
    "axs[0].set_title('PMF for each joint')\n",
    "axs[0].set_ylabel('probability')\n",
    "axs[0].set_xlabel('percent of lambdas voting 1')\n",
    "axs[1].plot(pmf,'b.')\n",
    "axs[1].set_title('PMF for each joint, second visualization')\n",
    "axs[1].set_ylabel('probability')\n",
    "axs[1].set_xlabel('assignment')\n",
    "axs[2].hist(sample_matrix[:,-1])\n",
    "axs[2].set_title('Class balance')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregate pixels with label model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "L_train = sample_matrix[:,:-1] # N x 5\n",
    "L_dev = sample_matrix_test[:,:-1] # M x 5\n",
    "Y_dev = sample_matrix_test[:,-1] # M\n",
    "\n",
    "est_thetas, est_pmf, P_train, P_dev = run_seg_label_model(L_train, L_dev, Y_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare to previous weak supervision model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empirical_p_Y = np.mean(Y_dev==1)\n",
    "fs_model, fs_ind_cond = flying_squid(sample_matrix[:,:-1], sample_matrix_test[:,:-1], m=5, v=1, cb=np.asarray([1-empirical_p_Y,empirical_p_Y]))\n",
    "print('Done with flying squid model.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare accuracies from binarized conditional probs on dev set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proposed_Y = binarize_conditionals(P_dev)\n",
    "fs_ind_Y = binarize_conditionals(fs_ind_cond)\n",
    "mv_Y, _, _ = compute_baselines(sample_matrix_test[:,:-1])\n",
    "lf1 = binarize_conditionals(sample_matrix_test[:,0])\n",
    "lf2 = binarize_conditionals(sample_matrix_test[:,1])\n",
    "lf3 = binarize_conditionals(sample_matrix_test[:,2])\n",
    "lf4 = binarize_conditionals(sample_matrix_test[:,3])\n",
    "lf5 = binarize_conditionals(sample_matrix_test[:,4])\n",
    "\n",
    "for preds, pred_name in zip([proposed_Y,fs_ind_Y,mv_Y,lf1,lf2,lf3,lf4,lf5],\n",
    "                            ['Proposed','Flying Squid','Majority vote','LF1','LF2','LF3','LF4','LF5']):\n",
    "    print('\\t',pred_name,'f1:',f1_score(sample_matrix_test[:,-1],preds[:]))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use PGM estimation with real data\n",
    "\n",
    "To run with real data, you need to define:\n",
    "- X_train: list of N training images; each image is size X x Y x D; each pixel should be [0,1]\n",
    "- L_train: list of N training noisy labels; each noisy label is size 5 x X x Y x D; each pixel should be {0,1}\n",
    "- X_dev: list of M dev set images; each image is size X x Y x D; each pixel should be [0,1]\n",
    "- L_dev: list of M dev set noisy labels; each noisy label is size 5 x X x Y x D; each pixel should be {0,1}\n",
    "- Y_dev: list of M dev set GT labels; each GT label is size X x Y x D; each pixel should be {0,1}\n",
    "\n",
    "We will reformat the lists into following matrices:\n",
    "- L_train_mat: matrix of all training noisy labels; matrix is size (N * X * Y * D) x 5\n",
    "- L_dev_mat: matrix of all dev set noisy labels; matrix is size (M * X * Y * D) x 5\n",
    "- Y_dev_mat: matrix of all dev set ground truth labels; matrix is size (M * X * Y * D)\n",
    "\n",
    "Then use the label model + CRF code to generate the following results:\n",
    "- Y_apx_train: list of N approximated labels for train set; each label is size X x Y x D\n",
    "- Y_apx_dev: list of M approximated labels for dev set; each label is size X x Y x D\n",
    "\n",
    "Note: you can reformat code to use without dev set labels if they're not available\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start by reshaping the lists into matrices\n",
    "L_train_mat = np.hstack([l.reshape(5,-1) for l in L_train]).T\n",
    "L_dev_mat = np.hstack([l.reshape(5,-1) for l in L_dev]).T\n",
    "Y_dev_mat = np.squeeze(np.hstack([y.reshape(1,-1) for y in Y_dev]).T)\n",
    "\n",
    "L_train_mat[L_train_mat<1] = -1\n",
    "L_dev_mat[L_dev_mat<1] = -1\n",
    "Y_dev_mat[Y_dev_mat<1] = -1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Aggregate at the pixel level\n",
    "est_thetas, est_pmf, P_train_mat, P_dev_mat = run_seg_label_model(L_train_mat, L_dev_mat, Y_dev_mat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reformat matrices into lists\n",
    "P_train = []\n",
    "total_pix = 0\n",
    "for img in X_train:\n",
    "    pix_in_img = img.shape[0]*img.shape[1]*img.shape[2]\n",
    "    img_weak_pix = P_train_mat[total_pix:total_pix+pix_in_img]\n",
    "    img_weak_pix = img_weak_pix.reshape(img.shape)\n",
    "    P_train += [img_weak_pix]\n",
    "    total_pix += pix_in_img\n",
    "        \n",
    "P_dev = []\n",
    "total_pix = 0\n",
    "for img in X_dev:\n",
    "    pix_in_img = img.shape[0]*img.shape[1]*img.shape[2]\n",
    "    img_weak_pix = P_dev_mat[total_pix:total_pix+pix_in_img]\n",
    "    img_weak_pix = img_weak_pix.reshape(img.shape)\n",
    "    P_dev += [img_weak_pix]\n",
    "    total_pix += pix_in_img\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run CRF over probabilistic labels\n",
    "Y_apx_train, Y_apx_dev = run_crf(X_train, P_train, X_dev, P_dev, Y_dev, seed=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check dice for all images on dev set\n",
    "all_dice = {'LF 0':[],'LF 1':[],'LF 2':[],'LF 3':[],'LF 4':[], 'MV':[], 'Pred':[]}\n",
    "for img, weak, pred, strong in zip(X_dev,L_dev,Y_apx_dev,Y_dev):\n",
    "    for lf in range(weak.shape[0]):\n",
    "        dice = compute_dice(strong,weak[lf])\n",
    "        all_dice['LF '+str(lf)] += [dice]\n",
    "    mv = np.sum(weak,0)>(weak.shape[0]/2)\n",
    "    dice = compute_dice(strong,mv)\n",
    "    all_dice['MV'] += [dice]\n",
    "    dice = compute_dice(strong,pred)\n",
    "    all_dice['Pred'] += [dice]\n",
    "\n",
    "print([(k,np.mean(v),len(v)) for (k, v) in all_dice.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "segEnv",
   "language": "python",
   "name": "segenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
